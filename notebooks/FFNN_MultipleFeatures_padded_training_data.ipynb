{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import andi \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as kr\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.features import Q_measure_2D, Dist_distribution, Convex_hull\n",
    "from src.andi_tools import Andi_to_xy, group_by_length, group_similar_as\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2D =  pd.read_csv(\"../data_generation/test_2d.csv\").values[:,2:]\n",
    "list_2D_total = [all_2D[i][~np.isnan(all_2D[i])].reshape(1,-1) for i in np.arange(len(all_2D))]\n",
    "xdata, ydata = Andi_to_xy(list_2D_total)\n",
    "output_2d = pd.read_csv(\"../data_generation/label_2d.csv\").values[:,1]\n",
    "output = np.array([kr.utils.to_categorical(output_2d[i], num_classes=5) for i in range(len(output_2d))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_2D = pd.read_csv(\"../../../ANDI_data/data_2D_task2.csv\").values[:,1:]\n",
    "list_2D_total = [all_2D[i][~np.isnan(all_2D[i])].reshape(1,-1) for i in np.arange(len(all_2D))]\n",
    "xdata, ydata = Andi_to_xy(list_2D_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_2d = pd.read_csv(\"../../../ANDI_data/labels_2D_task2.csv\").values[:,2]\n",
    "output = np.array([kr.utils.to_categorical(output_2d[i], num_classes=5) for i in range(len(output_2d))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the training set and calculate the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the different features \n",
    "\n",
    "Q_list, lens = Q_measure_2D(xdata, ydata , 2)\n",
    "Q_list_2, lens_2 = Q_measure_2D(xdata, ydata , 8)\n",
    "Dist_list, lens_dist = Dist_distribution(xdata, ydata)\n",
    "Convex_list, conv_dist = Convex_hull(xdata, ydata, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the Q-values\n",
    "\n",
    "result = np.squeeze(np.array([np.pad(i,((0,0),(0,np.max(lens)-i.shape[1]))) for i in Q_list]),axis=1)\n",
    "result_2 = np.squeeze(np.array([np.pad(i,((0,0),(0,np.max(lens_2)-i.shape[1]))) for i in Q_list_2]),axis=1)\n",
    "result_3 = np.squeeze(np.array([np.pad(i,((0,0),(0,np.max(conv_dist)-i.shape[1]))) for i in Convex_list]),axis=1)\n",
    "result_d = np.array(Dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into a training and test set \n",
    "\n",
    "fraction = np.int(len(xdata)*0.75)\n",
    "\n",
    "result_train, result_test = result[:fraction], result[fraction:]\n",
    "result2_train, result2_test = result_2[:fraction], result_2[fraction:]\n",
    "result3_train, result3_test = result_3[:fraction], result_3[fraction:]\n",
    "resultd_train, resultd_test = result_d[:fraction], result_d[fraction:]\n",
    "\n",
    "output_train, output_test = output[:fraction], output[fraction:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "inputA = kr.layers.Input(shape=(result_train.shape[1],))\n",
    "inputB = kr.layers.Input(shape=(result2_train.shape[1],))\n",
    "inputC = kr.layers.Input(shape=(result3_train.shape[1],))\n",
    "inputD = kr.layers.Input(shape=(resultd_train.shape[1],))\n",
    "# Side A \n",
    "x1 =  kr.layers.Dense(32, activation=\"sigmoid\")(inputA)\n",
    "x1 =  kr.layers.Dense(32, activation=\"sigmoid\")(x1)\n",
    "x1 =  kr.Model(inputs=inputA, outputs=x1)\n",
    "# Side B\n",
    "x2 = kr.layers.Dense(32, activation=\"sigmoid\")(inputB)\n",
    "x2 = kr.layers.Dense(32, activation=\"sigmoid\")(x2)\n",
    "x2 = kr.Model(inputs=inputB, outputs=x2)\n",
    "# Side C\n",
    "x3 =  kr.layers.Dense(32, activation=\"sigmoid\")(inputC)\n",
    "x3 =  kr.layers.Dense(32, activation=\"sigmoid\")(x3)\n",
    "x3 =  kr.Model(inputs=inputC, outputs=x3)\n",
    "# Side D\n",
    "x4 =  kr.layers.Dense(32, activation=\"sigmoid\")(inputD)\n",
    "x4 =  kr.layers.Dense(32, activation=\"sigmoid\")(x4)\n",
    "x4 =  kr.Model(inputs=inputD, outputs=x4)\n",
    "# Combine side A en B\n",
    "combined = kr.layers.concatenate([x1.output, x2.output, x3.output, x4.output])\n",
    "z = kr.layers.Dense(4, activation=\"sigmoid\")(combined)\n",
    "z = kr.layers.Dense(output_train.shape[1], activation=\"softmax\")(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kr.Model(inputs=[x1.input, x2.input, x3.input, x4.input], outputs=z)\n",
    "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 0s 955us/step - loss: nan - accuracy: 0.2880\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 0s 237us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 0s 235us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 0s 208us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 0s 297us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 0s 249us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 0s 197us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 0s 187us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 0s 209us/step - loss: nan - accuracy: 0.2800\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 0s 198us/step - loss: nan - accuracy: 0.2800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x10a83af50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([result_train,result2_train,result3_train,resultd_train], output_train, epochs=10, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.064753496170044, 0.8159999847412109]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([result_test,result2_test,result3_test,resultd_test], output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"../saved_models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../saved_models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
