{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of trajectories \n",
    "Here we classify trajectory data based on the training datasets of labelled trajectories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.features import Q_measure, dist_distribution, convex_hull\n",
    "from src.andi_tools import Andi_to_xy\n",
    "from src.training import FeedForward\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trajectories from training dataset with labels  \n",
    "trajectories, labels = Andi_to_xy(\"data/test_2d.csv\",\"data/label_2d.csv\")\n",
    "\n",
    "# make labels in format we need \n",
    "labels = torch.FloatTensor(labels)\n",
    "labels = torch.max(labels, 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features of trajectories calculation \n",
    "List of all features is available in source code. We estimate Q measure of trajectories for different window size,\n",
    "distanceof jumps distributions. \n",
    "\n",
    "Comment: Q measure with different window size parameters gives redundant information about trajectory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate features of data-trajectory: e.g. Q measure with parameter of window size w\n",
    "\n",
    "w1 = 2 #parameter of window size  \n",
    "w2 = 4 #parameter of window size  \n",
    "w3 = 6 #parameter of window size  \n",
    "\n",
    "# run features function which calculates different features \n",
    "features =[Q_measure(trajectories, w1),\n",
    "            Q_measure(trajectories, w2),\n",
    "            Q_measure(trajectories, w3),\n",
    "          dist_distribution(trajectories)]\n",
    "\n",
    "# make features to be of the right size \n",
    "feature_sizes = [f.shape[1] for f in features]\n",
    "features = [torch.FloatTensor(f) for f in features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of neural network\n",
    "Model of training of neural network used: feed forward. \n",
    "\n",
    "We use cross entropy. \n",
    "We use Adam optimizer function for its robustness. The learning rate parameter lr is chosen by default to be lr=0.001. However performance of NN depends on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training network with certain parameters \n",
    "\n",
    "def train_net(lr_i):\n",
    "    model = FeedForward(feature_sizes)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr_i) # learning rate default is 0.01 \n",
    "\n",
    "    model.train()\n",
    "    epoch = 1000\n",
    "    for epoch in range(epoch):    \n",
    "        optimizer.zero_grad()    # Forward pass\n",
    "        y_pred = model(features)    # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), labels)\n",
    "   \n",
    "        #print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how learning rate influences neural network performance. \n",
    "In general, the learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 1.0\n",
      "0.256\n",
      "learning rate 0.5\n",
      "0.256\n",
      "learning rate 0.3333333333333333\n",
      "0.256\n",
      "learning rate 0.25\n",
      "0.256\n"
     ]
    }
   ],
   "source": [
    "lr_i = 0.001\n",
    "model = FeedForward(feature_sizes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # learning rate default is 0.01 \n",
    "\n",
    "train_net(lr_i)\n",
    "\n",
    "for ind in range(1,5):\n",
    "    lr_i = 1./ind\n",
    "    train_net(lr_i)\n",
    "    print('learning rate', lr_i)\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    epoch = 1000\n",
    "    for epoch in range(epoch):    \n",
    "        optimizer.zero_grad()    # Forward pass\n",
    "        y_pred = model(features)    # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), labels)\n",
    "   \n",
    "    #print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    after_train = criterion(y_pred,labels)\n",
    "    prediction = torch.max(y_pred, 1)[1]\n",
    "\n",
    "    \n",
    "    nonzero = np.count_nonzero(np.argmax(y_pred.detach().numpy(),axis=1)-labels.detach().numpy())\n",
    "    tot = len(np.argmax(y_pred.detach().numpy(),axis=1)-labels.detach().numpy())\n",
    "\n",
    "    print((tot-nonzero)/tot)\n",
    "    (tot-nonzero)/tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training with one parameter setting\n",
    "Previous simulations and model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForward(feature_sizes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # learning rate default is 0.01 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "epoch = 1000\n",
    "for epoch in range(epoch):    \n",
    "    optimizer.zero_grad()    # Forward pass\n",
    "    y_pred = model(features)    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), labels)\n",
    "   \n",
    "    #print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_train = criterion(y_pred,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.max(y_pred, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero = np.count_nonzero(np.argmax(y_pred.detach().numpy(),axis=1)-labels.detach().numpy())\n",
    "tot = len(np.argmax(y_pred.detach().numpy(),axis=1)-labels.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tot-nonzero)/tot\n",
    "\n",
    "# change the optimiser of the model and its parameters above \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "#0.456 for lr=0.01\n",
    "#0.33 for lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
